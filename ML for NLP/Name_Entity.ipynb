{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc75f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=\"On the evening of February 24, 2007, at Coimbatore, I had a very beautiful experience. As I got ready for meeting the first person out of twenty appointments, a wheel chair was in sight with a smiling person probably in his late fifties; unfortunately he has no hands and legs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347b3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "para2sent=sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6b9de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On the evening of February 24, 2007, at Coimbatore, I had a very beautiful experience.', 'As I got ready for meeting the first person out of twenty appointments, a wheel chair was in sight with a smiling person probably in his late fifties; unfortunately he has no hands and legs.']\n"
     ]
    }
   ],
   "source": [
    "print(para2sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f96817b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'the', 'evening', 'of', 'February', '24', ',', '2007', ',', 'at', 'Coimbatore', ',', 'I', 'had', 'a', 'very', 'beautiful', 'experience', '.']\n",
      "['As', 'I', 'got', 'ready', 'for', 'meeting', 'the', 'first', 'person', 'out', 'of', 'twenty', 'appointments', ',', 'a', 'wheel', 'chair', 'was', 'in', 'sight', 'with', 'a', 'smiling', 'person', 'probably', 'in', 'his', 'late', 'fifties', ';', 'unfortunately', 'he', 'has', 'no', 'hands', 'and', 'legs', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(para2sent)):\n",
    "    sent2word=word_tokenize(para2sent[i])\n",
    "    print(sent2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4da249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ManiKandan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75cefdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ManiKandan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On the evening of February 24, 2007, at Coimbatore, I had a very beautiful experience.',\n",
       " 'As I got ready for meeting the first person out of twenty appointments, a wheel chair was in sight with a smiling person probably in his late fifties; unfortunately he has no hands and legs.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para2sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(para2sent)):\n",
    "    sent2word=word_tokenize(para2sent[i])\n",
    "    pos=nltk.pos_tag(sent2word)\n",
    "    nltk.ne_chunk(pos).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"The Periya Kovil, also known as the Brihadeeswarar Temple in Thanjavur, was built by the Chola king Raja Raja Chola I between 1003 and 1010 CE, showcasing the architectural brilliance of the Chola dynasty with its massive granite structure, towering vimana, and intricate sculptures, and today it stands as a UNESCO World Heritage Site and one of the finest examples of Tamil temple architecture.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f998183",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2w=nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c490b652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Periya', 'Kovil', ',', 'also', 'known', 'as', 'the', 'Brihadeeswarar', 'Temple', 'in', 'Thanjavur', ',', 'was', 'built', 'by', 'the', 'Chola', 'king', 'Raja', 'Raja', 'Chola', 'I', 'between', '1003', 'and', '1010', 'CE', ',', 'showcasing', 'the', 'architectural', 'brilliance', 'of', 'the', 'Chola', 'dynasty', 'with', 'its', 'massive', 'granite', 'structure', ',', 'towering', 'vimana', ',', 'and', 'intricate', 'sculptures', ',', 'and', 'today', 'it', 'stands', 'as', 'a', 'UNESCO', 'World', 'Heritage', 'Site', 'and', 'one', 'of', 'the', 'finest', 'examples', 'of', 'Tamil', 'temple', 'architecture', '.']\n"
     ]
    }
   ],
   "source": [
    "print(s2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ef67a76",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nltk\u001b[38;5;241m.\u001b[39mne_chunk(s2w)\n",
      "File \u001b[1;32mc:\\Users\\ManiKandan\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\__init__.py:184\u001b[0m, in \u001b[0;36mne_chunk\u001b[1;34m(tagged_tokens, binary)\u001b[0m\n\u001b[0;32m    182\u001b[0m     chunker_pickle \u001b[38;5;241m=\u001b[39m _MULTICLASS_NE_CHUNKER\n\u001b[0;32m    183\u001b[0m chunker \u001b[38;5;241m=\u001b[39m load(chunker_pickle)\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mparse(tagged_tokens)\n",
      "File \u001b[1;32mc:\\Users\\ManiKandan\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\named_entity.py:127\u001b[0m, in \u001b[0;36mNEChunkParser.parse\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m    Each token should be a pos-tagged word\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m     tagged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tagger\u001b[38;5;241m.\u001b[39mtag(tokens)\n\u001b[0;32m    128\u001b[0m     tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tagged_to_parse(tagged)\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32mc:\\Users\\ManiKandan\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py:61\u001b[0m, in \u001b[0;36mSequentialBackoffTagger.tag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     59\u001b[0m tags \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens)):\n\u001b[1;32m---> 61\u001b[0m     tags\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag_one(tokens, i, tags))\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(tokens, tags))\n",
      "File \u001b[1;32mc:\\Users\\ManiKandan\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py:81\u001b[0m, in \u001b[0;36mSequentialBackoffTagger.tag_one\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m     79\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tagger \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_taggers:\n\u001b[1;32m---> 81\u001b[0m     tag \u001b[38;5;241m=\u001b[39m tagger\u001b[38;5;241m.\u001b[39mchoose_tag(tokens, index, history)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ManiKandan\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py:647\u001b[0m, in \u001b[0;36mClassifierBasedTagger.choose_tag\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_tag\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, index, history):\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;66;03m# Use our feature detector to get the featureset.\u001b[39;00m\n\u001b[1;32m--> 647\u001b[0m     featureset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_detector(tokens, index, history)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;66;03m# Use the classifier to pick a tag.  If a cutoff probability\u001b[39;00m\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;66;03m# was specified, then check that the tag's probability is\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;66;03m# higher than that cutoff first; otherwise, return None.\u001b[39;00m\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cutoff_prob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ManiKandan\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py:694\u001b[0m, in \u001b[0;36mClassifierBasedTagger.feature_detector\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_detector\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, index, history):\n\u001b[0;32m    685\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;124;03m    Return the feature detector that this tagger uses to generate\u001b[39;00m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;124;03m    featuresets for its classifier.  The feature detector is a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;124;03m    See ``classifier()``\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_detector(tokens, index, history)\n",
      "File \u001b[1;32mc:\\Users\\ManiKandan\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\named_entity.py:90\u001b[0m, in \u001b[0;36mNEChunkParserTagger._feature_detector\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m     88\u001b[0m     nextpos \u001b[38;5;241m=\u001b[39m tokens[index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     89\u001b[0m     nextnextword \u001b[38;5;241m=\u001b[39m tokens[index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m---> 90\u001b[0m     nextnextpos \u001b[38;5;241m=\u001b[39m tokens[index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# 89.6\u001b[39;00m\n\u001b[0;32m     93\u001b[0m features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: shape(word),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape+prevtag\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m+\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    110\u001b[0m }\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "nltk.ne_chunk(s2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1aecf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt=nltk.pos_tag(s2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db86f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(pt).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac3054ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##observation\n",
    "##Before passing the word to ne_chunk , it should be pos_tag. Pos_tag result will be passed as a parameter to ne_chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb025d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
